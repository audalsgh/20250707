## 머신러닝 및 학습 유형 정리표
머신러닝 : 과거 데이터로부터 학습해, 보지 않은 데이터에도 높은 성능으로 일반화된 결과를 도출 = 다양한 상황에 적응 가능
| 학습 유형         | 레이블 필요 여부 | 정의 및 목적                                                                           | 대표 알고리즘                                                         | 활용 예시                | 장점                                      | 단점                                   |
| ---------------- | --------- | --------------------------------------------------------------------------------- | --------------------------------------------------------------- | ---------------------------------------- | --------------------------------------- | ------------------------------------ |
| **지도학습**      | 필요        | 입력(x)과 정답(y) 쌍이 주어진 <br>데이터로 학습<br>올바른 출력 예측<br>**목표**: 정확한 값 예측 및 분류                 | 회귀: 선형 회귀, 다항 회귀<br>분류: 로지스틱 회귀, SVM, 결정 트리, 랜덤 포레스트, k-NN      | 주택 가격 예측,<br>스팸 메일 분류 | - 명확한 평가 지표로 성능 측정 가능<br>- 높은 예측 정확도 달성 | - 라벨링 비용 및 시간 소요<br>- 라벨 오류 시 성능 저하  |
| **비지도학습**     | 불필요       | 라벨 없는 입력(x)만으로 데이터의 숨은 구조나 패턴 발견<br>**목표**: 데이터 클러스터링, 차원 축소, 분포 추정               | 클러스터링: k-평균, 계층적 클러스터링<br>차원 축소: PCA, t-SNE, UMAP<br>밀도 추정: GMM | 고객 세그멘테이션,<br>이상치 탐지  | - 라벨링 불필요로 비용 절감<br>- 데이터 구조 이해에 유용     | - 결과 해석이 주관적일 수 있음<br>- 평가 지표 정의 어려움 |
| **머신러닝 (ML)** | 불필요        | 데이터를 통해 패턴 학습<br>예측·분류·패턴 인식 수행<br>**목표**: 과거 데이터로부터 모델을 일반화하여 새로운 데이터에도 높은 성능 달성 |  Q-러닝 : 손실 함수(loss)를 최소화하고 모델 파라미터(가중치)를 최적화하는 과정                                                               | 다양한 산업 전반 적용         | - 문제에 맞는 알고리즘 선택 가능<br>- 자동화된 의사결정 지원   | - 데이터 품질 및 양에 민감<br>- 과적합 위험         |

클러스터링 = 군집화를 의미
> ※ 실제 문제에 따라 준지도학습, 강화학습 등 다른 학습 방법도 함께 고려해야 합니다.

## 머신러닝 학습 방법 비교
1. 지도학습 : 정답이 있는 훈련 데이터로 학습<br>
장점 : 성능 평가 기능이 좋고, 예측 정확도가 높으며, 검증된 알고리즘이 많아 해석 가능성이 높다.<br>
단점 : 라벨링 비용이 초기에 많이 들고, 학습되지 않은 새로운 상황에 대응하긴 어렵다.<br>
ex) 신호등을 보고 무슨 색상인지 정답 데이터와 비교.

2. 비지도학습 : 데이터 구조와 패턴을 발견하는 학습<br>
ex) 마트에서 우유 옆에 맥주를 놓으니 잘 팔리더라. (정해져있지 않은 사실을 발견함)<br>

3. 강화학습 : 라벨링은 필요없고, 행동→보상→정책 업데이트 과정에서 (보상 신호)만 사용<br>
즉, “이 행동이 좋다(+보상), 나쁘다(–보상)” 정도의 피드백만 있으면 학습 가능<br>
- 환경 모델(또는 시뮬레이터)로 에이전트 행동에 보상을 줄 수 있어야 하고<br>
- 어떤 행동에 얼마나 보상을 줄지 "보상 함수"를 설계해야한다.<br>

이렇게만 갖춰지면, RL 에이전트는 스스로 최적의 행동 전략을 찾아간다.<br>
**다양한 상황에 적응 가능하지만, 안정성이 떨어진다.**<br>
ex) 격자형 미로(Grid World)에서 Q-러닝 에이전트가 목표 지점에 도달하도록 학습,<br> 충분한 반복 후 에이전트는 최단 경로로 G에 도달하는 Q값을 학습하게 됨.
